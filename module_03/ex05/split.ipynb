{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb662dc2",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd0d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as e:\n",
    "    print(f\"Error: Required libraries not found: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26cfcc",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c17aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded: 398 rows, 31 columns\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(filepath: str) -> pd.DataFrame:\n",
    "    path = Path(filepath)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {filepath}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "try:\n",
    "    train_df = load_dataset('../resources/Train_knight.csv')\n",
    "    print(f\"✓ Dataset loaded: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a0f33",
   "metadata": {},
   "source": [
    "## Why Split the Data?\n",
    "\n",
    "We split data into **Training** and **Validation** sets to:\n",
    "1. **Train** the model on one portion\n",
    "2. **Test** it on unseen data to check if it generalizes well\n",
    "\n",
    "### Common Split Ratios:\n",
    "| Training | Validation | Use Case |\n",
    "|----------|------------|----------|\n",
    "| 80% | 20% | Most common, good balance |\n",
    "| 70% | 30% | When you want more validation data |\n",
    "| 90% | 10% | When you have lots of data |\n",
    "\n",
    "**We'll use 80/20 split** - industry standard for most ML tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c1f1c",
   "metadata": {},
   "source": [
    "## Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f564c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df: pd.DataFrame, train_ratio: float = 0.8, random_state: int = 42) -> tuple:\n",
    "    \"\"\"\n",
    "    Randomly split a DataFrame into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to split\n",
    "        train_ratio: Proportion for training (0.8 = 80%)\n",
    "        random_state: Seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (training_df, validation_df)\n",
    "    \"\"\"\n",
    "    df_shuffled = df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    split_index = int(len(df_shuffled) * train_ratio)\n",
    "    \n",
    "    training_df = df_shuffled[:split_index]\n",
    "    validation_df = df_shuffled[split_index:]\n",
    "    \n",
    "    return training_df, validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "446dd40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 398 rows\n",
      "Training set: 318 rows (79.9%)\n",
      "Validation set: 80 rows (20.1%)\n"
     ]
    }
   ],
   "source": [
    "training_df, validation_df = split_dataset(train_df, train_ratio=0.8)\n",
    "\n",
    "print(f\"Original dataset: {len(train_df)} rows\")\n",
    "print(f\"Training set: {len(training_df)} rows ({len(training_df)/len(train_df)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(validation_df)} rows ({len(validation_df)/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178b3f7",
   "metadata": {},
   "source": [
    "## Verify Class Distribution\n",
    "Make sure both sets have similar proportions of Jedi/Sith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc81cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in Training set:\n",
      "knight\n",
      "Sith    0.632075\n",
      "Jedi    0.367925\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in Validation set:\n",
      "knight\n",
      "Sith    0.5625\n",
      "Jedi    0.4375\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in Training set:\")\n",
    "print(training_df['knight'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in Validation set:\")\n",
    "print(validation_df['knight'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a6776",
   "metadata": {},
   "source": [
    "## Save the Split Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "804e423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Files saved:\n",
      "  - Training_knight.csv\n",
      "  - Validation_knight.csv\n"
     ]
    }
   ],
   "source": [
    "training_df.to_csv('Training_knight.csv', index=False)\n",
    "validation_df.to_csv('Validation_knight.csv', index=False)\n",
    "\n",
    "print(\"✓ Files saved:\")\n",
    "print(\"  - Training_knight.csv\")\n",
    "print(\"  - Validation_knight.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5dfee1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **80% Training**: Enough data for the model to learn patterns\n",
    "- **20% Validation**: Enough data to reliably test model performance\n",
    "- **Industry standard**: Widely used, good balance between learning and testing\n",
    "- **Prevents overfitting**: Model can't just memorize the training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdc (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
